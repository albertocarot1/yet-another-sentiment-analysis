{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Sentiment Analysis Data Analysis\n",
    "\n",
    "The goal of this project is to create a general purpose sentiment analysis tool, where a user can input any text, and get its sentiment in three possible ways:\n",
    "* positive or negative\n",
    "* 1 to 5 stars\n",
    "* float number between 0 and 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from matplotlib import pyplot as plt\n",
    "from pandas.api.types import is_float_dtype\n",
    "from transformers import AutoTokenizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# os.environ['HF_DATASETS_OFFLINE'] = \"1\"\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "sns.set_style(\"whitegrid\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Readily available datasets are used, from the _datasets_ library. The selected datasets are free to use for any purpose (even commercial)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: sst/default\n",
      "Found cached dataset sst (/home/alberto/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff)\n",
      "100%|██████████| 3/3 [00:00<00:00, 562.77it/s]\n",
      "Found cached dataset imdb (/home/alberto/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n",
      "100%|██████████| 3/3 [00:00<00:00, 435.32it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"int\") to str",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 5\u001B[0m\n\u001B[1;32m      3\u001B[0m sa_datasets[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msst\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m load_dataset(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msst\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      4\u001B[0m sa_datasets[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mds_imdb\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m load_dataset(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimdb\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 5\u001B[0m sa_datasets[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmovrat\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mload_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmovie_rationales\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      6\u001B[0m sa_datasets[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtweet\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m load_dataset(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtweet_eval\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msentiment\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      7\u001B[0m sa_datasets[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrotten\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m load_dataset(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrotten_tomatoes\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Documents/anaconda3/envs/nlp-env/lib/python3.9/site-packages/datasets/load.py:1719\u001B[0m, in \u001B[0;36mload_dataset\u001B[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, **config_kwargs)\u001B[0m\n\u001B[1;32m   1716\u001B[0m ignore_verifications \u001B[38;5;241m=\u001B[39m ignore_verifications \u001B[38;5;129;01mor\u001B[39;00m save_infos\n\u001B[1;32m   1718\u001B[0m \u001B[38;5;66;03m# Create a dataset builder\u001B[39;00m\n\u001B[0;32m-> 1719\u001B[0m builder_instance \u001B[38;5;241m=\u001B[39m \u001B[43mload_dataset_builder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1720\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1721\u001B[0m \u001B[43m    \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1722\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1723\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_files\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_files\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1724\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1725\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfeatures\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfeatures\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1726\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1727\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdownload_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1728\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1729\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_auth_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_auth_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1730\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mconfig_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1731\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1733\u001B[0m \u001B[38;5;66;03m# Return iterable dataset in case of streaming\u001B[39;00m\n\u001B[1;32m   1734\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m streaming:\n",
      "File \u001B[0;32m~/Documents/anaconda3/envs/nlp-env/lib/python3.9/site-packages/datasets/load.py:1523\u001B[0m, in \u001B[0;36mload_dataset_builder\u001B[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, **config_kwargs)\u001B[0m\n\u001B[1;32m   1520\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(error_msg)\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# Instantiate the dataset builder\u001B[39;00m\n\u001B[0;32m-> 1523\u001B[0m builder_instance: DatasetBuilder \u001B[38;5;241m=\u001B[39m \u001B[43mbuilder_cls\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1524\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1525\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconfig_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1526\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1527\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_files\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_files\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1528\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mhash\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mhash\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfeatures\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfeatures\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1530\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_auth_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_auth_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1531\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mbuilder_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1532\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mconfig_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1533\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1535\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m builder_instance\n",
      "File \u001B[0;32m~/Documents/anaconda3/envs/nlp-env/lib/python3.9/site-packages/datasets/builder.py:1292\u001B[0m, in \u001B[0;36mGeneratorBasedBuilder.__init__\u001B[0;34m(self, writer_batch_size, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1291\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, writer_batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m-> 1292\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1293\u001B[0m     \u001B[38;5;66;03m# Batch size used by the ArrowWriter\u001B[39;00m\n\u001B[1;32m   1294\u001B[0m     \u001B[38;5;66;03m# It defines the number of samples that are kept in memory before writing them\u001B[39;00m\n\u001B[1;32m   1295\u001B[0m     \u001B[38;5;66;03m# and also the length of the arrow chunks\u001B[39;00m\n\u001B[1;32m   1296\u001B[0m     \u001B[38;5;66;03m# None means that the ArrowWriter will use its default value\u001B[39;00m\n\u001B[1;32m   1297\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_writer_batch_size \u001B[38;5;241m=\u001B[39m writer_batch_size \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mDEFAULT_WRITER_BATCH_SIZE\n",
      "File \u001B[0;32m~/Documents/anaconda3/envs/nlp-env/lib/python3.9/site-packages/datasets/builder.py:312\u001B[0m, in \u001B[0;36mDatasetBuilder.__init__\u001B[0;34m(self, cache_dir, config_name, hash, base_path, info, features, use_auth_token, repo_id, data_files, data_dir, name, **config_kwargs)\u001B[0m\n\u001B[1;32m    309\u001B[0m \u001B[38;5;66;03m# prepare info: DatasetInfo are a standardized dataclass across all datasets\u001B[39;00m\n\u001B[1;32m    310\u001B[0m \u001B[38;5;66;03m# Prefill datasetinfo\u001B[39;00m\n\u001B[1;32m    311\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m info \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 312\u001B[0m     info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_exported_dataset_info\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    313\u001B[0m     info\u001B[38;5;241m.\u001B[39mupdate(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_info())\n\u001B[1;32m    314\u001B[0m     info\u001B[38;5;241m.\u001B[39mbuilder_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname\n",
      "File \u001B[0;32m~/Documents/anaconda3/envs/nlp-env/lib/python3.9/site-packages/datasets/builder.py:412\u001B[0m, in \u001B[0;36mDatasetBuilder.get_exported_dataset_info\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    400\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_exported_dataset_info\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DatasetInfo:\n\u001B[1;32m    401\u001B[0m     \u001B[38;5;124;03m\"\"\"Empty DatasetInfo if doesn't exist\u001B[39;00m\n\u001B[1;32m    402\u001B[0m \n\u001B[1;32m    403\u001B[0m \u001B[38;5;124;03m    Example:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    410\u001B[0m \u001B[38;5;124;03m    ```\u001B[39;00m\n\u001B[1;32m    411\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 412\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_all_exported_dataset_infos\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mname, DatasetInfo())\n",
      "File \u001B[0;32m~/Documents/anaconda3/envs/nlp-env/lib/python3.9/site-packages/datasets/builder.py:398\u001B[0m, in \u001B[0;36mDatasetBuilder.get_all_exported_dataset_infos\u001B[0;34m(cls)\u001B[0m\n\u001B[1;32m    385\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[1;32m    386\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_all_exported_dataset_infos\u001B[39m(\u001B[38;5;28mcls\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DatasetInfosDict:\n\u001B[1;32m    387\u001B[0m     \u001B[38;5;124;03m\"\"\"Empty dict if doesn't exist\u001B[39;00m\n\u001B[1;32m    388\u001B[0m \n\u001B[1;32m    389\u001B[0m \u001B[38;5;124;03m    Example:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    396\u001B[0m \u001B[38;5;124;03m    ```\u001B[39;00m\n\u001B[1;32m    397\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 398\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mDatasetInfosDict\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_directory\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_imported_module_dir\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/anaconda3/envs/nlp-env/lib/python3.9/site-packages/datasets/info.py:370\u001B[0m, in \u001B[0;36mDatasetInfosDict.from_directory\u001B[0;34m(cls, dataset_infos_dir)\u001B[0m\n\u001B[1;32m    368\u001B[0m     dataset_metadata \u001B[38;5;241m=\u001B[39m DatasetMetadata\u001B[38;5;241m.\u001B[39mfrom_readme(Path(dataset_infos_dir) \u001B[38;5;241m/\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mREADME.md\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    369\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdataset_info\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m dataset_metadata:\n\u001B[0;32m--> 370\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_metadata\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset_metadata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    371\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(dataset_infos_dir, config\u001B[38;5;241m.\u001B[39mDATASETDICT_INFOS_FILENAME)):\n\u001B[1;32m    372\u001B[0m     \u001B[38;5;66;03m# this is just to have backward compatibility with dataset_infos.json files\u001B[39;00m\n\u001B[1;32m    373\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(dataset_infos_dir, config\u001B[38;5;241m.\u001B[39mDATASETDICT_INFOS_FILENAME), encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n",
      "File \u001B[0;32m~/Documents/anaconda3/envs/nlp-env/lib/python3.9/site-packages/datasets/info.py:396\u001B[0m, in \u001B[0;36mDatasetInfosDict.from_metadata\u001B[0;34m(cls, dataset_metadata)\u001B[0m\n\u001B[1;32m    387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m(\n\u001B[1;32m    388\u001B[0m         {\n\u001B[1;32m    389\u001B[0m             dataset_info_yaml_dict\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconfig_name\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdefault\u001B[39m\u001B[38;5;124m\"\u001B[39m): DatasetInfo\u001B[38;5;241m.\u001B[39m_from_yaml_dict(\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    393\u001B[0m         }\n\u001B[1;32m    394\u001B[0m     )\n\u001B[1;32m    395\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 396\u001B[0m     dataset_info \u001B[38;5;241m=\u001B[39m \u001B[43mDatasetInfo\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_from_yaml_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset_metadata\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdataset_info\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    397\u001B[0m     dataset_info\u001B[38;5;241m.\u001B[39mconfig_name \u001B[38;5;241m=\u001B[39m dataset_metadata[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdataset_info\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconfig_name\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdefault\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    398\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m({dataset_info\u001B[38;5;241m.\u001B[39mconfig_name: dataset_info})\n",
      "File \u001B[0;32m~/Documents/anaconda3/envs/nlp-env/lib/python3.9/site-packages/datasets/info.py:332\u001B[0m, in \u001B[0;36mDatasetInfo._from_yaml_dict\u001B[0;34m(cls, yaml_data)\u001B[0m\n\u001B[1;32m    330\u001B[0m yaml_data \u001B[38;5;241m=\u001B[39m copy\u001B[38;5;241m.\u001B[39mdeepcopy(yaml_data)\n\u001B[1;32m    331\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m yaml_data\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfeatures\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 332\u001B[0m     yaml_data[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfeatures\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mFeatures\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_from_yaml_list\u001B[49m\u001B[43m(\u001B[49m\u001B[43myaml_data\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfeatures\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    333\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m yaml_data\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msplits\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    334\u001B[0m     yaml_data[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msplits\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m SplitDict\u001B[38;5;241m.\u001B[39m_from_yaml_list(yaml_data[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msplits\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n",
      "File \u001B[0;32m~/Documents/anaconda3/envs/nlp-env/lib/python3.9/site-packages/datasets/features/features.py:1745\u001B[0m, in \u001B[0;36mFeatures._from_yaml_list\u001B[0;34m(cls, yaml_data)\u001B[0m\n\u001B[1;32m   1742\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1743\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected a dict or a list but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(obj)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mobj\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 1745\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mfrom_dict(\u001B[43mfrom_yaml_inner\u001B[49m\u001B[43m(\u001B[49m\u001B[43myaml_data\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/Documents/anaconda3/envs/nlp-env/lib/python3.9/site-packages/datasets/features/features.py:1741\u001B[0m, in \u001B[0;36mFeatures._from_yaml_list.<locals>.from_yaml_inner\u001B[0;34m(obj)\u001B[0m\n\u001B[1;32m   1739\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj, \u001B[38;5;28mlist\u001B[39m):\n\u001B[1;32m   1740\u001B[0m     names \u001B[38;5;241m=\u001B[39m [_feature\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m _feature \u001B[38;5;129;01min\u001B[39;00m obj]\n\u001B[0;32m-> 1741\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {name: from_yaml_inner(_feature) \u001B[38;5;28;01mfor\u001B[39;00m name, _feature \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(names, obj)}\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1743\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected a dict or a list but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(obj)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mobj\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Documents/anaconda3/envs/nlp-env/lib/python3.9/site-packages/datasets/features/features.py:1741\u001B[0m, in \u001B[0;36m<dictcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m   1739\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj, \u001B[38;5;28mlist\u001B[39m):\n\u001B[1;32m   1740\u001B[0m     names \u001B[38;5;241m=\u001B[39m [_feature\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m _feature \u001B[38;5;129;01min\u001B[39;00m obj]\n\u001B[0;32m-> 1741\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {name: \u001B[43mfrom_yaml_inner\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_feature\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m name, _feature \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(names, obj)}\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1743\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected a dict or a list but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(obj)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mobj\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Documents/anaconda3/envs/nlp-env/lib/python3.9/site-packages/datasets/features/features.py:1736\u001B[0m, in \u001B[0;36mFeatures._from_yaml_list.<locals>.from_yaml_inner\u001B[0;34m(obj)\u001B[0m\n\u001B[1;32m   1734\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: snakecase_to_camelcase(obj[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype\u001B[39m\u001B[38;5;124m\"\u001B[39m])}\n\u001B[1;32m   1735\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfrom_yaml_inner\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdtype\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1737\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1738\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: snakecase_to_camelcase(_type), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39munsimplify(obj)[_type]}\n",
      "File \u001B[0;32m~/Documents/anaconda3/envs/nlp-env/lib/python3.9/site-packages/datasets/features/features.py:1738\u001B[0m, in \u001B[0;36mFeatures._from_yaml_list.<locals>.from_yaml_inner\u001B[0;34m(obj)\u001B[0m\n\u001B[1;32m   1736\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m from_yaml_inner(obj[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1738\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: snakecase_to_camelcase(_type), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[43munsimplify\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m[_type]}\n\u001B[1;32m   1739\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj, \u001B[38;5;28mlist\u001B[39m):\n\u001B[1;32m   1740\u001B[0m     names \u001B[38;5;241m=\u001B[39m [_feature\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m _feature \u001B[38;5;129;01min\u001B[39;00m obj]\n",
      "File \u001B[0;32m~/Documents/anaconda3/envs/nlp-env/lib/python3.9/site-packages/datasets/features/features.py:1706\u001B[0m, in \u001B[0;36mFeatures._from_yaml_list.<locals>.unsimplify\u001B[0;34m(feature)\u001B[0m\n\u001B[1;32m   1704\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(feature\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclass_label\u001B[39m\u001B[38;5;124m\"\u001B[39m), \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(feature[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclass_label\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m), \u001B[38;5;28mdict\u001B[39m):\n\u001B[1;32m   1705\u001B[0m     label_ids \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msorted\u001B[39m(feature[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclass_label\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m-> 1706\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m label_ids \u001B[38;5;129;01mand\u001B[39;00m label_ids \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mrange\u001B[39m(\u001B[43mlabel_ids\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m)):\n\u001B[1;32m   1707\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1708\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mClassLabel expected a value for all label ids [0:\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlabel_ids[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m] but some ids are missing.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1709\u001B[0m         )\n\u001B[1;32m   1710\u001B[0m     feature[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclass_label\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m [feature[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclass_label\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m][label_id] \u001B[38;5;28;01mfor\u001B[39;00m label_id \u001B[38;5;129;01min\u001B[39;00m label_ids]\n",
      "\u001B[0;31mTypeError\u001B[0m: can only concatenate str (not \"int\") to str"
     ]
    }
   ],
   "source": [
    "sa_datasets: Dict[str, Dataset] = {}\n",
    "\n",
    "sa_datasets['sst'] = load_dataset(\"sst\")\n",
    "sa_datasets['ds_imdb'] = load_dataset(\"imdb\")\n",
    "sa_datasets['movrat'] = load_dataset(\"movie_rationales\")\n",
    "sa_datasets['tweet'] = load_dataset(\"tweet_eval\", \"sentiment\")\n",
    "sa_datasets['rotten'] = load_dataset(\"rotten_tomatoes\")\n",
    "sa_datasets['amzpol'] = load_dataset(\"amazon_polarity\")\n",
    "\n",
    "# sa_datasets['amazon'] = load_dataset(\"amazon_reviews_multi\", \"en\")   # cannot use for money ever\n",
    "# sa_datasets['yelp'] = load_dataset(\"yelp_review_full\") # cannot use for money ever\n",
    "# sa_datasets['financ'] = load_dataset(\"financial_phrasebank\", \"sentences_75agree\")  # Must ask for commercial license"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Text length analysis\n",
    "We want to compare the length of texts for each dataset (in terms of tokens). All the data splits are taken into account for the exploratory data analysis.\n",
    "The tokenizers used are the ones of the transformers that performed well from the literature in the sentiment analysis task, but that are also relatively light for prediction. See https://paperswithcode.com/task/sentiment-analysis for details."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max_num_tokens = 512\n",
    "tokenizer_xlm = AutoTokenizer.from_pretrained(\"xlm-roberta-base\", model_max_length=max_num_tokens, truncation=True)\n",
    "tokenizer_xlnet = AutoTokenizer.from_pretrained(\"xlnet-large-cased\", model_max_length=max_num_tokens, truncation=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_num_tokens(tokens):\n",
    "    return len(tokens.encodings[0].ids)\n",
    "\n",
    "sa_dataframe = pd.DataFrame()\n",
    "\n",
    "for ds_name, ds in sa_datasets.items():\n",
    "    print(f'Processing {ds_name} dataset.')\n",
    "    # Get all for analysis: train, test, val etc.\n",
    "    # df = concatenate_datasets([ds[split] for split in ds.column_names.keys()]).to_pandas()\n",
    "    datasets_to_concat = [ds[split] for split in ds.column_names.keys()]\n",
    "    ds_full = concatenate_datasets(datasets_to_concat)\n",
    "\n",
    "    # Get all relevant text in 'text' column\n",
    "    if ds_name in ['sst', 'financ']:\n",
    "        ds_full = ds_full.rename_column(\"sentence\", \"text\")\n",
    "    elif ds_name == 'movrat':\n",
    "        ds_full = ds_full.rename_column(\"review\", \"text\")\n",
    "    elif ds_name == 'amzpol':\n",
    "        df = ds_full.to_pandas()\n",
    "        titles_with_punct = df['title'].str.endswith(('.','!','?'))\n",
    "        df.loc[~titles_with_punct, 'title'] += '. '\n",
    "        df.loc[titles_with_punct, 'title'] += ' '\n",
    "        df['text'] = df['title'] + df['content']\n",
    "        ds_full = Dataset.from_pandas(df)\n",
    "\n",
    "\n",
    "    print(\"Extracting tokens...\")\n",
    "    ds_full = ds_full.map(lambda rows: tokenizer_xlm(rows['text']), batched=True)\n",
    "    # ds_full = ds_full.map(lambda rows: tokenizer_xlnet(rows['text']), batched=True)\n",
    "\n",
    "    df: pd.DataFrame = ds_full.to_pandas()\n",
    "\n",
    "    df_num_tokens = pd.DataFrame({ds_name: df['input_ids'].map(lambda x: x.size)})\n",
    "    sa_dataframe = pd.concat([sa_dataframe,df_num_tokens], axis=1)\n",
    "\n",
    "    print(f'Dataset {ds_name} processed.')\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sa_dataframe.describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sa_dataframe.count().plot.bar()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can observe how the amazon reviews dataset is by orders of magnitude the largest one. This means that using all the datapoints from that dataset might bias the results towards consumer reviews."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sa_dataframe.plot.box(figsize=(12,12), showmeans=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sa_dataframe.plot.box(subplots=True, figsize=(20,8), showmeans=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The boxplots show how two of the movie reviews dataset are much more verbose than the text in the other datasets.\n",
    "\n",
    "Because of the limitation in maximum token length of the chosen transformer models, it is better to limit text to 512 tokens, truncating the text for longer reviews.\n",
    "\n",
    "An alternative to truncation, is to split evenly long text, and getting more than one data point from a single movie review."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sa_dataframe.plot.density(subplots=True, sharex=False, figsize=(12, 16))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Kernel Density Estimate leads to similar conclusions as the boxplots, and indicates that the movrat dataset is the one with the longest texts. Simply truncating the texts might lead to a large loss of information in this case."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Label distribution analysis\n",
    "\n",
    "We want to see for the default splits of the datasets, how many points are positive and negative reviews.\n",
    "The final goal is to train a general purpose sentiment analysis tool, so we want a model that performs similarly on all datasets used for training, or at least on all domains covered by the datasets (movies, consumer reviews, tweets)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sa_dataframe = pd.DataFrame()\n",
    "\n",
    "# For multiple plots\n",
    "fig, axs = plt.subplots(2, 3)\n",
    "max_col = 3\n",
    "current_col = 0\n",
    "current_row = 0\n",
    "\n",
    "for ds_name, ds in sa_datasets.items():\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    for split in ds.column_names.keys():\n",
    "        if split not in ['train', 'test', 'validation']:\n",
    "            continue\n",
    "        df_split = ds[split].to_pandas()\n",
    "\n",
    "        if is_float_dtype(df_split.label.dtype):\n",
    "            df_counts = pd.DataFrame(df_split.label.value_counts(bins=2))\n",
    "        else:\n",
    "            df_counts = pd.DataFrame(df_split.label.value_counts())\n",
    "        df_counts.rename(columns={\"label\": split}, inplace=True)\n",
    "\n",
    "        df = pd.concat([df_counts,df], axis=1)\n",
    "    df = df.T\n",
    "\n",
    "    ax = df.plot.bar(figsize=(20,16), title=ds_name,ax=axs[current_row,current_col])\n",
    "\n",
    "    current_col += 1\n",
    "    if current_col == max_col:\n",
    "        current_col = 0\n",
    "        current_row += 1\n",
    "\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Apart from the _tweet_ dataset, all the other dataset have proportional data between different labels in the data splits.\n",
    "\n",
    "Splitting the long texts in _movrat_, _imdb_ and _amzpol_ will return in different sized datasets, as we do not know yet how many 512 tokens long data point will be created by splitting the data.\n",
    "\n",
    "TODO To not waste most of the text by truncating the content longer than 512 tokens."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
