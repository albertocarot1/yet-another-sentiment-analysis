{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Regression training\n",
    "\n",
    "Since the sentiment analysis tool aims to be able to return binary, 5 stars, or decimal results, the problem will be treated as a regression problem."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alberto/Documents/anaconda3/envs/nlp-env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Dict\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "import wandb"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33malbertocarot1\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.13.9"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/mnt/DATA/Documents/PycharmProjects/IMDB-Experiments/wandb/run-20230207_163806-j0f4ainz</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href=\"https://wandb.ai/albertocarot1/sentiment-analysis/runs/j0f4ainz\" target=\"_blank\">drawn-terrain-2</a></strong> to <a href=\"https://wandb.ai/albertocarot1/sentiment-analysis\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href=\"https://wandb.ai/albertocarot1/sentiment-analysis\" target=\"_blank\">https://wandb.ai/albertocarot1/sentiment-analysis</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href=\"https://wandb.ai/albertocarot1/sentiment-analysis/runs/j0f4ainz\" target=\"_blank\">https://wandb.ai/albertocarot1/sentiment-analysis/runs/j0f4ainz</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/albertocarot1/sentiment-analysis/runs/j0f4ainz?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>",
      "text/plain": "<wandb.sdk.wandb_run.Run at 0x7f7fb5db51c0>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['HF_DATASETS_OFFLINE'] = \"1\"\n",
    "wandb.init(project=\"sentiment-analysis\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: sst/default\n",
      "Found cached dataset sst (/home/alberto/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff)\n",
      "100%|██████████| 3/3 [00:00<00:00, 535.03it/s]\n",
      "Found cached dataset imdb (/home/alberto/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n",
      "100%|██████████| 3/3 [00:00<00:00, 318.76it/s]\n",
      "Downloading readme: 100%|██████████| 6.65k/6.65k [00:00<00:00, 4.82MB/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"int\") to str",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 5\u001B[0m\n\u001B[1;32m      3\u001B[0m sa_datasets[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msst\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m load_dataset(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msst\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      4\u001B[0m sa_datasets[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mds_imdb\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m load_dataset(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimdb\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 5\u001B[0m sa_datasets[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmovrat\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mload_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmovie_rationales\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      6\u001B[0m sa_datasets[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtweet\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m load_dataset(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtweet_eval\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msentiment\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      7\u001B[0m sa_datasets[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrotten\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m load_dataset(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrotten_tomatoes\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Documents/anaconda3/envs/nlp-env/lib/python3.9/site-packages/datasets/load.py:1719\u001B[0m, in \u001B[0;36mload_dataset\u001B[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, **config_kwargs)\u001B[0m\n\u001B[1;32m   1716\u001B[0m ignore_verifications \u001B[38;5;241m=\u001B[39m ignore_verifications \u001B[38;5;129;01mor\u001B[39;00m save_infos\n\u001B[1;32m   1718\u001B[0m \u001B[38;5;66;03m# Create a dataset builder\u001B[39;00m\n\u001B[0;32m-> 1719\u001B[0m builder_instance \u001B[38;5;241m=\u001B[39m \u001B[43mload_dataset_builder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1720\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1721\u001B[0m \u001B[43m    \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1722\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1723\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_files\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_files\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1724\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1725\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfeatures\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfeatures\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1726\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1727\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdownload_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1728\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1729\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_auth_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_auth_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1730\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mconfig_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1731\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1733\u001B[0m \u001B[38;5;66;03m# Return iterable dataset in case of streaming\u001B[39;00m\n\u001B[1;32m   1734\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m streaming:\n",
      "File \u001B[0;32m~/Documents/anaconda3/envs/nlp-env/lib/python3.9/site-packages/datasets/load.py:1523\u001B[0m, in \u001B[0;36mload_dataset_builder\u001B[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, **config_kwargs)\u001B[0m\n\u001B[1;32m   1520\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(error_msg)\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# Instantiate the dataset builder\u001B[39;00m\n\u001B[0;32m-> 1523\u001B[0m builder_instance: DatasetBuilder \u001B[38;5;241m=\u001B[39m \u001B[43mbuilder_cls\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1524\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1525\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconfig_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1526\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1527\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_files\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_files\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1528\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mhash\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mhash\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfeatures\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfeatures\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1530\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_auth_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_auth_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1531\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mbuilder_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1532\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mconfig_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1533\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1535\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m builder_instance\n",
      "File \u001B[0;32m~/Documents/anaconda3/envs/nlp-env/lib/python3.9/site-packages/datasets/builder.py:1292\u001B[0m, in \u001B[0;36mGeneratorBasedBuilder.__init__\u001B[0;34m(self, writer_batch_size, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1291\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, writer_batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m-> 1292\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1293\u001B[0m     \u001B[38;5;66;03m# Batch size used by the ArrowWriter\u001B[39;00m\n\u001B[1;32m   1294\u001B[0m     \u001B[38;5;66;03m# It defines the number of samples that are kept in memory before writing them\u001B[39;00m\n\u001B[1;32m   1295\u001B[0m     \u001B[38;5;66;03m# and also the length of the arrow chunks\u001B[39;00m\n\u001B[1;32m   1296\u001B[0m     \u001B[38;5;66;03m# None means that the ArrowWriter will use its default value\u001B[39;00m\n\u001B[1;32m   1297\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_writer_batch_size \u001B[38;5;241m=\u001B[39m writer_batch_size \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mDEFAULT_WRITER_BATCH_SIZE\n",
      "File \u001B[0;32m~/Documents/anaconda3/envs/nlp-env/lib/python3.9/site-packages/datasets/builder.py:312\u001B[0m, in \u001B[0;36mDatasetBuilder.__init__\u001B[0;34m(self, cache_dir, config_name, hash, base_path, info, features, use_auth_token, repo_id, data_files, data_dir, name, **config_kwargs)\u001B[0m\n\u001B[1;32m    309\u001B[0m \u001B[38;5;66;03m# prepare info: DatasetInfo are a standardized dataclass across all datasets\u001B[39;00m\n\u001B[1;32m    310\u001B[0m \u001B[38;5;66;03m# Prefill datasetinfo\u001B[39;00m\n\u001B[1;32m    311\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m info \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 312\u001B[0m     info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_exported_dataset_info\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    313\u001B[0m     info\u001B[38;5;241m.\u001B[39mupdate(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_info())\n\u001B[1;32m    314\u001B[0m     info\u001B[38;5;241m.\u001B[39mbuilder_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname\n",
      "File \u001B[0;32m~/Documents/anaconda3/envs/nlp-env/lib/python3.9/site-packages/datasets/builder.py:412\u001B[0m, in \u001B[0;36mDatasetBuilder.get_exported_dataset_info\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    400\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_exported_dataset_info\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DatasetInfo:\n\u001B[1;32m    401\u001B[0m     \u001B[38;5;124;03m\"\"\"Empty DatasetInfo if doesn't exist\u001B[39;00m\n\u001B[1;32m    402\u001B[0m \n\u001B[1;32m    403\u001B[0m \u001B[38;5;124;03m    Example:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    410\u001B[0m \u001B[38;5;124;03m    ```\u001B[39;00m\n\u001B[1;32m    411\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 412\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_all_exported_dataset_infos\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mname, DatasetInfo())\n",
      "File \u001B[0;32m~/Documents/anaconda3/envs/nlp-env/lib/python3.9/site-packages/datasets/builder.py:398\u001B[0m, in \u001B[0;36mDatasetBuilder.get_all_exported_dataset_infos\u001B[0;34m(cls)\u001B[0m\n\u001B[1;32m    385\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[1;32m    386\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_all_exported_dataset_infos\u001B[39m(\u001B[38;5;28mcls\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DatasetInfosDict:\n\u001B[1;32m    387\u001B[0m     \u001B[38;5;124;03m\"\"\"Empty dict if doesn't exist\u001B[39;00m\n\u001B[1;32m    388\u001B[0m \n\u001B[1;32m    389\u001B[0m \u001B[38;5;124;03m    Example:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    396\u001B[0m \u001B[38;5;124;03m    ```\u001B[39;00m\n\u001B[1;32m    397\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 398\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mDatasetInfosDict\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_directory\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_imported_module_dir\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/anaconda3/envs/nlp-env/lib/python3.9/site-packages/datasets/info.py:370\u001B[0m, in \u001B[0;36mDatasetInfosDict.from_directory\u001B[0;34m(cls, dataset_infos_dir)\u001B[0m\n\u001B[1;32m    368\u001B[0m     dataset_metadata \u001B[38;5;241m=\u001B[39m DatasetMetadata\u001B[38;5;241m.\u001B[39mfrom_readme(Path(dataset_infos_dir) \u001B[38;5;241m/\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mREADME.md\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    369\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdataset_info\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m dataset_metadata:\n\u001B[0;32m--> 370\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_metadata\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset_metadata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    371\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(dataset_infos_dir, config\u001B[38;5;241m.\u001B[39mDATASETDICT_INFOS_FILENAME)):\n\u001B[1;32m    372\u001B[0m     \u001B[38;5;66;03m# this is just to have backward compatibility with dataset_infos.json files\u001B[39;00m\n\u001B[1;32m    373\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(dataset_infos_dir, config\u001B[38;5;241m.\u001B[39mDATASETDICT_INFOS_FILENAME), encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n",
      "File \u001B[0;32m~/Documents/anaconda3/envs/nlp-env/lib/python3.9/site-packages/datasets/info.py:396\u001B[0m, in \u001B[0;36mDatasetInfosDict.from_metadata\u001B[0;34m(cls, dataset_metadata)\u001B[0m\n\u001B[1;32m    387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m(\n\u001B[1;32m    388\u001B[0m         {\n\u001B[1;32m    389\u001B[0m             dataset_info_yaml_dict\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconfig_name\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdefault\u001B[39m\u001B[38;5;124m\"\u001B[39m): DatasetInfo\u001B[38;5;241m.\u001B[39m_from_yaml_dict(\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    393\u001B[0m         }\n\u001B[1;32m    394\u001B[0m     )\n\u001B[1;32m    395\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 396\u001B[0m     dataset_info \u001B[38;5;241m=\u001B[39m \u001B[43mDatasetInfo\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_from_yaml_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset_metadata\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdataset_info\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    397\u001B[0m     dataset_info\u001B[38;5;241m.\u001B[39mconfig_name \u001B[38;5;241m=\u001B[39m dataset_metadata[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdataset_info\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconfig_name\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdefault\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    398\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m({dataset_info\u001B[38;5;241m.\u001B[39mconfig_name: dataset_info})\n",
      "File \u001B[0;32m~/Documents/anaconda3/envs/nlp-env/lib/python3.9/site-packages/datasets/info.py:332\u001B[0m, in \u001B[0;36mDatasetInfo._from_yaml_dict\u001B[0;34m(cls, yaml_data)\u001B[0m\n\u001B[1;32m    330\u001B[0m yaml_data \u001B[38;5;241m=\u001B[39m copy\u001B[38;5;241m.\u001B[39mdeepcopy(yaml_data)\n\u001B[1;32m    331\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m yaml_data\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfeatures\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 332\u001B[0m     yaml_data[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfeatures\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mFeatures\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_from_yaml_list\u001B[49m\u001B[43m(\u001B[49m\u001B[43myaml_data\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfeatures\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    333\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m yaml_data\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msplits\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    334\u001B[0m     yaml_data[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msplits\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m SplitDict\u001B[38;5;241m.\u001B[39m_from_yaml_list(yaml_data[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msplits\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n",
      "File \u001B[0;32m~/Documents/anaconda3/envs/nlp-env/lib/python3.9/site-packages/datasets/features/features.py:1745\u001B[0m, in \u001B[0;36mFeatures._from_yaml_list\u001B[0;34m(cls, yaml_data)\u001B[0m\n\u001B[1;32m   1742\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1743\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected a dict or a list but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(obj)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mobj\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 1745\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mfrom_dict(\u001B[43mfrom_yaml_inner\u001B[49m\u001B[43m(\u001B[49m\u001B[43myaml_data\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/Documents/anaconda3/envs/nlp-env/lib/python3.9/site-packages/datasets/features/features.py:1741\u001B[0m, in \u001B[0;36mFeatures._from_yaml_list.<locals>.from_yaml_inner\u001B[0;34m(obj)\u001B[0m\n\u001B[1;32m   1739\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj, \u001B[38;5;28mlist\u001B[39m):\n\u001B[1;32m   1740\u001B[0m     names \u001B[38;5;241m=\u001B[39m [_feature\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m _feature \u001B[38;5;129;01min\u001B[39;00m obj]\n\u001B[0;32m-> 1741\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {name: from_yaml_inner(_feature) \u001B[38;5;28;01mfor\u001B[39;00m name, _feature \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(names, obj)}\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1743\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected a dict or a list but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(obj)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mobj\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Documents/anaconda3/envs/nlp-env/lib/python3.9/site-packages/datasets/features/features.py:1741\u001B[0m, in \u001B[0;36m<dictcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m   1739\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj, \u001B[38;5;28mlist\u001B[39m):\n\u001B[1;32m   1740\u001B[0m     names \u001B[38;5;241m=\u001B[39m [_feature\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m _feature \u001B[38;5;129;01min\u001B[39;00m obj]\n\u001B[0;32m-> 1741\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {name: \u001B[43mfrom_yaml_inner\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_feature\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m name, _feature \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(names, obj)}\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1743\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected a dict or a list but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(obj)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mobj\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Documents/anaconda3/envs/nlp-env/lib/python3.9/site-packages/datasets/features/features.py:1736\u001B[0m, in \u001B[0;36mFeatures._from_yaml_list.<locals>.from_yaml_inner\u001B[0;34m(obj)\u001B[0m\n\u001B[1;32m   1734\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: snakecase_to_camelcase(obj[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype\u001B[39m\u001B[38;5;124m\"\u001B[39m])}\n\u001B[1;32m   1735\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfrom_yaml_inner\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdtype\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1737\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1738\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: snakecase_to_camelcase(_type), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39munsimplify(obj)[_type]}\n",
      "File \u001B[0;32m~/Documents/anaconda3/envs/nlp-env/lib/python3.9/site-packages/datasets/features/features.py:1738\u001B[0m, in \u001B[0;36mFeatures._from_yaml_list.<locals>.from_yaml_inner\u001B[0;34m(obj)\u001B[0m\n\u001B[1;32m   1736\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m from_yaml_inner(obj[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1738\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: snakecase_to_camelcase(_type), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[43munsimplify\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m[_type]}\n\u001B[1;32m   1739\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj, \u001B[38;5;28mlist\u001B[39m):\n\u001B[1;32m   1740\u001B[0m     names \u001B[38;5;241m=\u001B[39m [_feature\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m _feature \u001B[38;5;129;01min\u001B[39;00m obj]\n",
      "File \u001B[0;32m~/Documents/anaconda3/envs/nlp-env/lib/python3.9/site-packages/datasets/features/features.py:1706\u001B[0m, in \u001B[0;36mFeatures._from_yaml_list.<locals>.unsimplify\u001B[0;34m(feature)\u001B[0m\n\u001B[1;32m   1704\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(feature\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclass_label\u001B[39m\u001B[38;5;124m\"\u001B[39m), \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(feature[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclass_label\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m), \u001B[38;5;28mdict\u001B[39m):\n\u001B[1;32m   1705\u001B[0m     label_ids \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msorted\u001B[39m(feature[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclass_label\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m-> 1706\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m label_ids \u001B[38;5;129;01mand\u001B[39;00m label_ids \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mrange\u001B[39m(\u001B[43mlabel_ids\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m)):\n\u001B[1;32m   1707\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1708\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mClassLabel expected a value for all label ids [0:\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlabel_ids[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m] but some ids are missing.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1709\u001B[0m         )\n\u001B[1;32m   1710\u001B[0m     feature[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclass_label\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m [feature[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclass_label\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m][label_id] \u001B[38;5;28;01mfor\u001B[39;00m label_id \u001B[38;5;129;01min\u001B[39;00m label_ids]\n",
      "\u001B[0;31mTypeError\u001B[0m: can only concatenate str (not \"int\") to str"
     ]
    }
   ],
   "source": [
    "sa_datasets: Dict[str, Dataset] = {}\n",
    "\n",
    "sa_datasets['sst'] = load_dataset(\"sst\")\n",
    "sa_datasets['ds_imdb'] = load_dataset(\"imdb\")\n",
    "sa_datasets['movrat'] = load_dataset(\"movie_rationales\")\n",
    "sa_datasets['tweet'] = load_dataset(\"tweet_eval\", \"sentiment\")\n",
    "sa_datasets['rotten'] = load_dataset(\"rotten_tomatoes\")\n",
    "sa_datasets['amzpol'] = load_dataset(\"amazon_polarity\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "max_num_tokens = 512\n",
    "starting_model = \"xlm-roberta-base\"  # \"xlnet-large-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(starting_model, model_max_length=max_num_tokens, truncation=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(starting_model, num_labels=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def preprocess(samples, dataset_name):\n",
    "    samples = tokenizer(samples[\"text\"], truncation=True, padding=\"max_length\", max_length=max_num_tokens)\n",
    "\n",
    "    if dataset_name == \"tweet\":\n",
    "        if samples[\"label\"] == 1:\n",
    "            samples[\"label\"] = 0.5\n",
    "        elif samples[\"label\"] == 2:\n",
    "            samples[\"label\"] = 1\n",
    "\n",
    "    samples[\"label\"] = float(samples[\"label\"])\n",
    "    return samples\n",
    "\n",
    "\n",
    "for ds_name, ds in sa_datasets.items():\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    for split in ds.column_names.keys():\n",
    "        if split not in ['train', 'test', 'validation']:\n",
    "            continue\n",
    "        # Get all relevant text in 'text' column and remove useless columns\n",
    "        if ds_name in ['sst', 'financ']:\n",
    "            ds[split] = ds[split].rename_column(\"sentence\", \"text\")\n",
    "            ds[split] = ds[split].remove_columns([\"tokens\", \"tree\"])\n",
    "        elif ds_name == 'movrat':\n",
    "            ds[split] = ds[split].rename_column(\"review\", \"text\")\n",
    "            ds[split] = ds[split].remove_columns(\"evidences\")\n",
    "        elif ds_name == 'amzpol':\n",
    "            df = ds[split].to_pandas()\n",
    "            titles_with_punct = df['title'].str.endswith(('.', '!', '?'))\n",
    "            df.loc[~titles_with_punct, 'title'] += '. '\n",
    "            df.loc[titles_with_punct, 'title'] += ' '\n",
    "            df['text'] = df['title'] + df['content']\n",
    "            ds[split] = Dataset.from_pandas(df)\n",
    "\n",
    "        ds[split] = ds[split].map(preprocess, fn_kwargs={'dataset_name': ds_name})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def compute_metrics_for_regression(eval_pred):\n",
    "#     logits, labels = eval_pred\n",
    "#     labels = labels.reshape(-1, 1)\n",
    "#\n",
    "#     mse = mean_squared_error(labels, logits)\n",
    "#     mae = mean_absolute_error(labels, logits)\n",
    "#     r2 = r2_score(labels, logits)\n",
    "#     single_squared_errors = ((logits - labels).flatten()**2).tolist()\n",
    "#\n",
    "#     # Compute accuracy\n",
    "#     # Based on the fact that the rounded score = true score only if |single_squared_errors| < 0.5\n",
    "#     accuracy = sum([1 for e in single_squared_errors if e < 0.25]) / len(single_squared_errors)\n",
    "#\n",
    "#     return {\"mse\": mse, \"mae\": mae, \"r2\": r2, \"accuracy\": accuracy}"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
